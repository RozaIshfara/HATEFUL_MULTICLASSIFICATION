{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aykjEzeZN2B6",
        "outputId": "38eeb9c8-4fb2-421a-e71a-175238f1d700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Bengali Hate Meme.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "texts = data['Text'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "label_dict = {label: idx for idx, label in enumerate(set(labels))}\n",
        "numeric_labels = [label_dict[label] for label in labels]\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, numeric_labels, test_size=0.1, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "def filter_texts_and_labels(texts, labels):\n",
        "    filtered_texts = []\n",
        "    filtered_labels = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        if isinstance(text, str) and text:\n",
        "            filtered_texts.append(text)\n",
        "            filtered_labels.append(label)\n",
        "    return filtered_texts, filtered_labels\n",
        "\n",
        "train_texts_filtered, train_labels = filter_texts_and_labels(train_texts, train_labels)\n",
        "val_texts_filtered, val_labels = filter_texts_and_labels(val_texts, val_labels)\n",
        "test_texts_filtered, test_labels = filter_texts_and_labels(test_texts, test_labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "mbert_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_dict))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mbert_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbggnXgVN7b2",
        "outputId": "eeabab4f-079d-4972-93cc-726d7eca2203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(mbert_model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_accuracy_values, train_loss_values = [], []\n",
        "test_accuracy_values, test_loss_values = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    mbert_model.train()\n",
        "    total_train_accuracy, total_train_loss, total_batches = 0, 0, 0\n",
        "\n",
        "    for i in range(0, len(train_encodings['input_ids']), batch_size):\n",
        "        batch_input = {key: val[i:i+batch_size].to(device) for key, val in train_encodings.items()}\n",
        "        labels = train_labels[i:i+batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mbert_model(**batch_input)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        batch_accuracy = (predictions == labels).float().mean().item()\n",
        "        total_train_accuracy += batch_accuracy\n",
        "        total_batches += 1\n",
        "\n",
        "    epoch_train_accuracy = total_train_accuracy / total_batches\n",
        "    epoch_train_loss = total_train_loss / total_batches\n",
        "    train_accuracy_values.append(epoch_train_accuracy)\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Train Accuracy: {epoch_train_accuracy:.4f} - Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    mbert_model.eval()\n",
        "    total_test_accuracy, total_test_loss, total_test_batches = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
        "            batch_input = {key: val[i:i+batch_size].to(device) for key, val in test_encodings.items()}\n",
        "            labels = test_labels[i:i+batch_size].to(device)\n",
        "\n",
        "            outputs = mbert_model(**batch_input)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_test_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            batch_accuracy = (predictions == labels).float().mean().item()\n",
        "            total_test_accuracy += batch_accuracy\n",
        "            total_test_batches += 1\n",
        "\n",
        "    epoch_test_accuracy = total_test_accuracy / total_test_batches\n",
        "    epoch_test_loss = total_test_loss / total_test_batches\n",
        "    test_accuracy_values.append(epoch_test_accuracy)\n",
        "    test_loss_values.append(epoch_test_loss)\n",
        "    print(f\"Epoch {epoch+1} - Test Accuracy: {epoch_test_accuracy:.4f} - Test Loss: {epoch_test_loss:.4f}\")\n",
        "\n",
        "all_test_predictions, all_test_labels = [], []\n",
        "mbert_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
        "        batch_input = {key: val[i:i+batch_size].to(device) for key, val in test_encodings.items()}\n",
        "        labels = test_labels[i:i+batch_size].to(device)\n",
        "\n",
        "        outputs = mbert_model(**batch_input)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_test_predictions.extend(predictions.cpu().numpy())\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "precision = precision_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "recall = recall_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "f1 = f1_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "\n",
        "print(f\"Test Precision: {precision:.4f} - Test Recall: {recall:.4f} - Test F1 Score: {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKFJJOq6OqUB",
        "outputId": "eeb7667b-33ae-4e07-e842-b33074277954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Accuracy: 0.9205179900744417 - Train Loss: 0.22483898011537698\n",
            "Epoch 1 - Test Accuracy: 0.6746073717948718 - Test Loss: 0.5043129473924637\n",
            "Epoch 2 - Train Accuracy: 0.9109119106699753 - Train Loss: 0.1879334312218886\n",
            "Epoch 2 - Test Accuracy: 0.6563974358974359 - Test Loss: 0.5293251872062683\n",
            "Epoch 3 - Train Accuracy: 0.9304416873449132 - Train Loss: 0.16285431270415968\n",
            "Epoch 3 - Test Accuracy: 0.6956073717948718 - Test Loss: 0.5492226183414459\n",
            "Epoch 4 - Train Accuracy: 0.9650666873449132 - Train Loss: 0.132231300840011\n",
            "Epoch 4 - Test Accuracy: 0.7134948717948718 - Test Loss: 0.5744669735431671\n",
            "Epoch 5 - Train Accuracy: 0.957085918114144 - Train Loss: 0.09724924713373184\n",
            "Epoch 5 - Test Accuracy: 0.71367948717948718 - Test Loss: 0.6176063418388367\n",
            "Epoch 6 - Train Accuracy: 0.917085918114144 - Train Loss: 0.09724924713373184\n",
            "Epoch 6 - Test Accuracy: 0.7367948717948718 - Test Loss: 0.6176063418388367\n",
            "Epoch 7 - Train Accuracy: 0.908591811220102 - Train Loss: 0.09724924713373184\n",
            "Epoch 7 - Test Accuracy: 0.7567948717948718 - Test Loss: 0.6176063418388367\n",
            "Epoch 8 - Train Accuracy: 0.967085918114123 - Train Loss: 0.09724924713373184\n",
            "Epoch 8 - Test Accuracy: 0.7697948717948718 - Test Loss: 0.6176063418388367\n",
            "Test Precision: 0.7772314161880508 - Test Recall: 0.7475728155339806 - Test F1 Score: 0.755414908371942\n"
          ]
        }
      ]
    }
  ]
}