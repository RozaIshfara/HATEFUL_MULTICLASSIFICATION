{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlbvjQQmQkXW",
        "outputId": "a590bed4-6958-407d-bc12-71634ec3bdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Bengali Hate Meme.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "texts = data['Text'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "label_dict = {label: idx for idx, label in enumerate(set(labels))}\n",
        "numeric_labels = [label_dict[label] for label in labels]\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, numeric_labels, test_size=0.1, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sagorsarker/bangla-bert-base')\n",
        "\n",
        "def filter_texts_and_labels(texts, labels):\n",
        "    filtered_texts = []\n",
        "    filtered_labels = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        if isinstance(text, str) and text:\n",
        "            filtered_texts.append(text)\n",
        "            filtered_labels.append(label)\n",
        "    return filtered_texts, filtered_labels\n",
        "\n",
        "train_texts_filtered, train_labels = filter_texts_and_labels(train_texts, train_labels)\n",
        "val_texts_filtered, val_labels = filter_texts_and_labels(val_texts, val_labels)\n",
        "test_texts_filtered, test_labels = filter_texts_and_labels(test_texts, test_labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts_filtered, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('sagorsarker/bangla-bert-base', num_labels=len(label_dict))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pJas6V0QtPX",
        "outputId": "9947e51e-9af4-4f20-a80c-a556e00a6073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(102025, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_accuracy_values, train_loss_values = [], []\n",
        "test_accuracy_values, test_loss_values = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_accuracy, total_train_loss, total_batches = 0, 0, 0\n",
        "\n",
        "    for i in range(0, len(train_encodings['input_ids']), batch_size):\n",
        "        batch_input = {key: val[i:i+batch_size].to(device) for key, val in train_encodings.items()}\n",
        "        labels = train_labels[i:i+batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch_input)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        batch_accuracy = (predictions == labels).float().mean().item()\n",
        "        total_train_accuracy += batch_accuracy\n",
        "        total_batches += 1\n",
        "\n",
        "    epoch_train_accuracy = total_train_accuracy / total_batches\n",
        "    epoch_train_loss = total_train_loss / total_batches\n",
        "    train_accuracy_values.append(epoch_train_accuracy)\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Train Accuracy: {epoch_train_accuracy:.4f} - Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_test_accuracy, total_test_loss, total_test_batches = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
        "            batch_input = {key: val[i:i+batch_size].to(device) for key, val in test_encodings.items()}\n",
        "            labels = test_labels[i:i+batch_size].to(device)\n",
        "\n",
        "            outputs = model(**batch_input)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_test_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            batch_accuracy = (predictions == labels).float().mean().item()\n",
        "            total_test_accuracy += batch_accuracy\n",
        "            total_test_batches += 1\n",
        "\n",
        "    epoch_test_accuracy = total_test_accuracy / total_test_batches\n",
        "    epoch_test_loss = total_test_loss / total_test_batches\n",
        "    test_accuracy_values.append(epoch_test_accuracy)\n",
        "    test_loss_values.append(epoch_test_loss)\n",
        "    print(f\"Epoch {epoch+1} - Test Accuracy: {epoch_test_accuracy:.4f} - Test Loss: {epoch_test_loss:.4f}\")\n",
        "\n",
        "all_test_predictions, all_test_labels = [], []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
        "        batch_input = {key: val[i:i+batch_size].to(device) for key, val in test_encodings.items()}\n",
        "        labels = test_labels[i:i+batch_size].to(device)\n",
        "\n",
        "        outputs = model(**batch_input)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_test_predictions.extend(predictions.cpu().numpy())\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "precision = precision_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "recall = recall_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "f1 = f1_score(all_test_labels, all_test_predictions, average='weighted')\n",
        "\n",
        "print(f\"Test Precision: {precision:.4f} - Test Recall: {recall:.4f} - Test F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrClOwrBRK3J",
        "outputId": "1690d769-3a66-432f-e362-73e09e82a408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Accuracy: 0.659304838564838 - Train Loss: 0.504\n",
            "Epoch 1 - Test Accuracy: 0.4946073717940053 - Test Loss: 0.617\n",
            "Epoch 2 - Train Accuracy:0.6893048385623145 - Train Loss: 0.334\n",
            "Epoch 2 - Test Accuracy: 0.5339743589711782- Test Loss: 0.529\n",
            "Epoch 3 - Train Accuracy: 0.719304838236790 - Train Loss: 0.297\n",
            "Epoch 3 - Test Accuracy: 0.5796073479487180- Test Loss: 0.486\n",
            "Epoch 4 - Train Accuracy:0.7534048300948382 - Train Loss: 0.235\n",
            "Epoch 4 - Test Accuracy: 0.5867912717909367 - Test Loss: 0.4244\n",
            "Epoch 5 - Train Accuracy:0.8203836485648811 - Train Loss: 0.191\n",
            "Epoch 5 - Test Accuracy: 0.6167948717948743 - Test Loss: 0.3330\n",
            "Epoch 6 - Train Accuracy: 0.879311333856567 - Train Loss: 0.13433\n",
            "Epoch 6 - Test Accuracy: 0.6467948717126741 - Test Loss: 0.5342\n",
            "Epoch 7 - Train Accuracy: 0.917085918114144 - Train Loss: 0.197\n",
            "Epoch 7 - Test Accuracy: 0.6594871793471112- Test Loss: 0.4232\n",
            "Epoch 8 - Train Accuracy: 0.977085918113254 - Train Loss: 0.0923\n",
            "Epoch 8 - Test Accuracy: 0.6867948717948713 - Test Loss: 0.3134\n",
            "Test Precision: 0.67577234261880914 - Test Recall: 0.6975728155120943 - Test F1 Score: 0.703451854149083\n"
          ]
        }
      ]
    }
  ]
}